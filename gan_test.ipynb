{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFGAN Minimal Sample\n",
    "## Author: Hiroshi Watanabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgan = tf.contrib.gan\n",
    "layers = tf.contrib.layers\n",
    "framework = tf.contrib.framework\n",
    "slim = tf.contrib.slim\n",
    "dataprovider = slim.dataset_data_provider.DatasetDataProvider\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fn(noise, weight_decay=2.5e-5, is_training=True):\n",
    "    f1 = framework.arg_scope(\n",
    "        [layers.fully_connected, layers.conv2d_transpose],\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=layers.batch_norm,\n",
    "        weights_regularizer=layers.l2_regularizer(weight_decay))\n",
    "    f2 = framework.arg_scope(\n",
    "        [layers.batch_norm],\n",
    "        is_training=is_training,\n",
    "        zero_debias_moving_mean=True)\n",
    "    with f1, f2:\n",
    "        net = layers.fully_connected(noise, 1024)\n",
    "        net = layers.fully_connected(net, 7 * 7 * 256)\n",
    "        net = tf.reshape(net, [-1, 7, 7, 256])\n",
    "        net = layers.conv2d_transpose(net, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d_transpose(net, 32, [4, 4], stride=2)\n",
    "        net = layers.conv2d(net, 1, 4, activation_fn=tf.tanh)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_fn(img, _, weight_decay=2.5e-5, is_training=True):\n",
    "    with framework.arg_scope(\n",
    "            [layers.conv2d, layers.fully_connected],\n",
    "            activation_fn=(lambda n: tf.nn.leaky_relu(n, alpha=0.01)),\n",
    "            weights_regularizer=layers.l2_regularizer(weight_decay),\n",
    "            biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "        net = layers.conv2d(img, 64, [4, 4], stride=2)\n",
    "        net = layers.conv2d(net, 128, [4, 4], stride=2)\n",
    "        net = layers.flatten(net)\n",
    "        with framework.arg_scope([layers.batch_norm], is_training=is_training):\n",
    "            net = layers.fully_connected(\n",
    "                net, 1024, normalizer_fn=layers.batch_norm)\n",
    "        return layers.linear(net, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_data(source, batch_size):\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),\n",
    "    }\n",
    "    datanum = sum(1 for _ in tf.python_io.tf_record_iterator(source))\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n",
    "    }\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "    reader = tf.TFRecordReader\n",
    "    dataset = slim.dataset.Dataset(source, reader, decoder, datanum, None)\n",
    "    provider = dataprovider(dataset, shuffle=True)\n",
    "    image, = provider.get(['image'])\n",
    "    image = (tf.cast(image, tf.float32) - 128.0) / 128.0\n",
    "    images = tf.train.batch([image], batch_size=batch_size)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"mnist.tfrecord\"\n",
    "#TRAIN_DATA = 'hiragana.tfrecord'\n",
    "#TRAIN_DATA = 'fontawesome.tfrecord'\n",
    "url=\"https://kaityo256.github.io/simple_tfgan/dataset/\"\n",
    "file=url+TRAIN_DATA\n",
    "!wget $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    real_images = provide_data(TRAIN_DATA, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model = tfgan.gan_model(\n",
    "    generator_fn,\n",
    "    discriminator_fn,\n",
    "    real_data=real_images,\n",
    "    generator_inputs=tf.random_normal([BATCH_SIZE, 64]))\n",
    "\n",
    "improved_wgan_loss = tfgan.gan_loss(\n",
    "    gan_model,\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    gradient_penalty_weight=1.0)\n",
    "\n",
    "generator_optimizer = tf.train.AdamOptimizer(0.001, beta1=0.5)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(0.0001, beta1=0.5)\n",
    "gan_train_ops = tfgan.gan_train_ops(\n",
    "    gan_model,\n",
    "    improved_wgan_loss,\n",
    "    generator_optimizer,\n",
    "    discriminator_optimizer)\n",
    "\n",
    "with tf.variable_scope('Generator', reuse=True):\n",
    "    eval_images = gan_model.generator_fn(\n",
    "        tf.random_normal([500, 64]),\n",
    "        is_training=False)\n",
    "\n",
    "visualizer = tfgan.eval.image_reshaper(eval_images[:20, ...], num_cols=10)\n",
    "\n",
    "train_step_fn = tfgan.get_sequential_train_steps()\n",
    "global_step = tf.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_STEPS = 201\n",
    "INTERVAL = 25\n",
    "with tf.train.SingularMonitoredSession() as sess:\n",
    "    for i in range(TOTAL_STEPS):\n",
    "        train_step_fn(sess, gan_train_ops, global_step,\n",
    "                        train_step_kwargs={})\n",
    "        if i % INTERVAL == 0:\n",
    "            digits_np = sess.run([visualizer])\n",
    "            plt.axis('off')\n",
    "            plt.imshow(np.squeeze(digits_np), cmap='gray')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
